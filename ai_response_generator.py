"""
AI –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —É–º–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ "—Ä–∞—Å—Å–∫–∞–∂–∏"
–ü—Ä–æ–µ–∫—Ç "–í—Ç–æ—Ä–æ–π –º–æ–∑–≥" - –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π –≥–æ–ª–æ—Å–æ–≤–æ–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç
"""

import json
import asyncio
from typing import Dict, List, Optional
from datetime import datetime, timedelta
import openai
from openai import AsyncOpenAI
import logging

logger = logging.getLogger(__name__)

class AIResponseGenerator:
    """AI –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —É–º–Ω—ã—Ö –∏ —á–µ–ª–æ–≤–µ—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤"""
    
    def __init__(self, api_key: str, base_url: str = "https://api.deepseek.com"):
        self.client = AsyncOpenAI(
            api_key=api_key,
            base_url=base_url
        )
        
        # –®–∞–±–ª–æ–Ω—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –æ—Ç–≤–µ—Ç–æ–≤
        self.response_templates = {
            'found_info': {
                'neutral': [
                    "–ù–∞–π–¥–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ {topic}:",
                    "–î–∞–Ω–Ω—ã–µ –æ {topic}:",
                    "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ {topic}:",
                    "–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ –ø–æ {topic}:"
                ]
            },
            'not_found': {
                'neutral': [
                    "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ {topic} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.",
                    "–î–∞–Ω–Ω—ã—Ö –æ {topic} –ø–æ–∫–∞ –Ω–µ—Ç.",
                    "–ó–∞–ø–∏—Å–µ–π –æ {topic} –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ.",
                    "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ {topic} –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç."
                ]
            },
            'recent_activity': [
                "–ö—Å—Ç–∞—Ç–∏, –Ω–µ–¥–∞–≤–Ω–æ —Ç—ã —É–ø–æ–º–∏–Ω–∞–ª:",
                "–ê –≤–æ—Ç —á—Ç–æ –±—ã–ª–æ –Ω–µ–¥–∞–≤–Ω–æ:",
                "–ö—Å—Ç–∞—Ç–∏, –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è:",
                "–ù–µ–¥–∞–≤–Ω–æ —Ç—ã –≥–æ–≤–æ—Ä–∏–ª:"
            ],
            'related_info': [
                "–¢–∞–∫–∂–µ —Å–≤—è–∑–∞–Ω–Ω–æ–µ —Å —ç—Ç–∏–º:",
                "–ï—â–µ –ø–æ —Ç–µ–º–µ:",
                "–ö—Å—Ç–∞—Ç–∏, –µ—Å—Ç—å —Å–≤—è–∑–∞–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:",
                "–¢–∞–∫–∂–µ —É–ø–æ–º–∏–Ω–∞–ª–æ—Å—å:"
            ]
        }
    
    def _create_system_prompt(self) -> str:
        """–°–æ–∑–¥–∞–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤"""
        return """–¢—ã - –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç "–ú–∏—Ä–∞". –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–∞–π–¥–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª–µ–∑–Ω—ã–µ, –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã.

–°–¢–ò–õ–¨ –û–ë–©–ï–ù–ò–Ø:
- –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π, –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–æ–Ω
- –ö–æ—Ä–æ—Ç–∫–∏–µ, –ø–æ–Ω—è—Ç–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
- –ú–∏–Ω–∏–º—É–º —ç–º–æ–¥–∑–∏ (—Ç–æ–ª—å–∫–æ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è)
- –õ–∏—á–Ω–æ–µ –æ–±—Ä–∞—â–µ–Ω–∏–µ "—Ç—ã"
- –§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥

–ü–†–ò–ù–¶–ò–ü–´:
1. –ï—Å–ª–∏ –µ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è - —Ä–∞—Å—Å–∫–∞–∂–∏ –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É
2. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç - —Å–æ–æ–±—â–∏ –æ–± —ç—Ç–æ–º –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ
3. –ò—Å–ø–æ–ª—å–∑—É–π –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
4. –ù–ï –¥–æ–±–∞–≤–ª—è–π —Ñ—Ä–∞–∑—ã —Ç–∏–ø–∞ "–≠—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ —Å–∏—Å—Ç–µ–º–µ"
5. –ù–ï –¥–æ–±–∞–≤–ª—è–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Ç–∏–ø–∞ "–†–µ–∫–æ–º–µ–Ω–¥—É—é –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –¥–∏–Ω–∞–º–∏–∫—É"
6. –ù–ï –ø—Ä–µ–¥–ª–∞–≥–∞–π –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è
7. –ù–ï —É–∫–∞–∑—ã–≤–∞–π –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π –æ—Ç–¥–µ–ª—å–Ω–æ
8. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π

–ó–ê–ü–†–ï–©–ï–ù–û:
- "–≠—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ —Å–∏—Å—Ç–µ–º–µ"
- "–†–µ–∫–æ–º–µ–Ω–¥—É—é –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –¥–∏–Ω–∞–º–∏–∫—É"
- "–†–µ–∫–æ–º–µ–Ω–¥—É—é –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥"
- "–°–æ–≤–µ—Ç—É—é –æ–±—Ä–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ"
- –õ—é–±—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏ —Å–æ–≤–µ—Ç—ã
- –£–ø–æ–º–∏–Ω–∞–Ω–∏—è –æ —Å–∏—Å—Ç–µ–º–µ —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö

–û–°–û–ë–´–ï –°–õ–£–ß–ê–ò:
- –î–ª—è –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (–≤–µ—Å, —Ä–∞—Å—Ö–æ–¥—ã, —Ä–µ–º–æ–Ω—Ç) - —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –æ—Ç–≤–µ—Ç
- –î–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ - –≥—Ä—É–ø–ø–∏—Ä—É–π –ø–æ –¥–∞—Ç–∞–º/–ø–µ—Ä–∏–æ–¥–∞–º
- –î–ª—è –ø–æ–∏—Å–∫–∞ –ª–æ–∫–∞—Ü–∏–π - –≤—ã–¥–µ–ª—è–π –º–µ—Å—Ç–∞
- –ò–∑–≤–ª–µ–∫–∞–π —á–∏—Å–ª–∞, –¥–∞—Ç—ã, —Å—É–º–º—ã –∫–æ–≥–¥–∞ –≤–æ–∑–º–æ–∂–Ω–æ

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (—Å—Ç—Ä–æ–≥–æ JSON):
{
    "response": "–æ—Å–Ω–æ–≤–Ω–æ–π –æ—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é",
    "tone": "caring|informative|encouraging",
    "has_info": true/false,
    "confidence": 0.8,
    "suggestions": []
}

–í–ê–ñ–ù–û:
- –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–º JSON
- –ù–µ –¥–æ–±–∞–≤–ª—è–π –Ω–∏–∫–∞–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∫—Ä–æ–º–µ JSON
- –ë—É–¥—å –∏—Å–∫—Ä–µ–Ω–Ω–µ–π –∏ –∑–∞–±–æ—Ç–ª–∏–≤–æ–π
- –ò—Å–ø–æ–ª—å–∑—É–π —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫
- –ú–∞–∫—Å–∏–º—É–º 3-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ –æ—Ç–≤–µ—Ç–µ
- suggestions –≤—Å–µ–≥–¥–∞ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø—É—Å—Ç—ã–º –º–∞—Å—Å–∏–≤–æ–º []"""
    
    def _format_search_data(self, search_results: Dict) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –¥–ª—è AI"""
        data_summary = []
        
        # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        if search_results['entities_found']:
            entities_text = []
            for entity in search_results['entities_found'][:3]:  # –¢–æ–ø-3 —Å—É—â–Ω–æ—Å—Ç–∏
                entities_text.append(f"{entity['name']} ({entity['type']})")
            data_summary.append(f"–ù–∞–π–¥–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏: {', '.join(entities_text)}")
        
        if search_results['entries_found']:
            entries_text = []
            for entry in search_results['entries_found'][:3]:  # –¢–æ–ø-3 –∑–∞–ø–∏—Å–∏
                # –û–±—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
                text = entry['original_text'][:100] + "..." if len(entry['original_text']) > 100 else entry['original_text']
                entries_text.append(f'"{text}"')
            data_summary.append(f"–ó–∞–ø–∏—Å–∏: {'; '.join(entries_text)}")
        
        if search_results['related_entities']:
            related_text = [e['name'] for e in search_results['related_entities'][:2]]
            data_summary.append(f"–°–≤—è–∑–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏: {', '.join(related_text)}")
        
        if search_results['recent_entries']:
            recent_text = []
            for entry in search_results['recent_entries'][:2]:
                text = entry['original_text'][:80] + "..." if len(entry['original_text']) > 80 else entry['original_text']
                recent_text.append(f'"{text}"')
            data_summary.append(f"–ù–µ–¥–∞–≤–Ω–∏–µ –∑–∞–ø–∏—Å–∏: {'; '.join(recent_text)}")
        
        return "\n".join(data_summary)
    
    def _extract_topic_from_query(self, query: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω—É—é —Ç–µ–º—É –∏–∑ –∑–∞–ø—Ä–æ—Å–∞"""
        # –£–±–∏—Ä–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–ª–æ–≤–∞
        stop_words = {'—Ä–∞—Å—Å–∫–∞–∂–∏', '–æ', '–ø—Ä–æ', '—á—Ç–æ', '–∑–Ω–∞–µ—à—å', '–ª–∏', '–ø–æ–∫–∞–∂–∏', '–µ—Å—Ç—å'}
        words = query.lower().split()
        topic_words = [w for w in words if w not in stop_words and len(w) > 2]
        
        if topic_words:
            return ' '.join(topic_words[:3])  # –ü–µ—Ä–≤—ã–µ 3 –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤–∞
        return query
    
    def _extract_structured_data(self, search_results: Dict) -> Dict:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –∑–∞–ø–∏—Å–µ–π"""
        structured_data = {
            'measurements': [],  # –≤–µ—Å: 79–∫–≥, –¥–∞—Ç–∞: –º–∞–π 2023
            'actions': [],       # –º–µ–Ω—è–ª –º–∞—Å–ª–æ, –¥–∞—Ç–∞: 01.01.2024
            'locations': [],     # –≥–¥–µ: —É–ª–∏—Ü–∞ –ë–∞—Ä–±–∞—à–æ–≤–∞
            'amounts': [],       # —Å—É–º–º—ã: 3000—Ä—É–±
            'dates': []          # –¥–∞—Ç—ã: 01.01.2024
        }
        
        # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∑–∞–ø–∏—Å–µ–π
        for entry in search_results.get('entries_found', []):
            text = entry.get('original_text', '')
            
            # –ò—â–µ–º –∏–∑–º–µ—Ä–µ–Ω–∏—è (–≤–µ—Å, –¥–∞–≤–ª–µ–Ω–∏–µ –∏ —Ç.–¥.)
            import re
            weight_match = re.search(r'(\d+)\s*(–∫–≥|–∫–∏–ª–æ–≥—Ä–∞–º–º)', text, re.IGNORECASE)
            if weight_match:
                structured_data['measurements'].append({
                    'type': 'weight',
                    'value': int(weight_match.group(1)),
                    'unit': 'kg',
                    'text': text
                })
            
            # –ò—â–µ–º —Å—É–º–º—ã
            amount_match = re.search(r'(\d+)\s*(—Ä—É–±|—Ä—É–±–ª–µ–π|‚ÇΩ)', text, re.IGNORECASE)
            if amount_match:
                structured_data['amounts'].append({
                    'amount': int(amount_match.group(1)),
                    'currency': 'RUB',
                    'text': text
                })
            
            # –ò—â–µ–º –¥–µ–π—Å—Ç–≤–∏—è
            if any(word in text.lower() for word in ['–º–µ–Ω—è–ª', '–ø–æ—á–∏–Ω–∏–ª', '—Ä–µ–º–æ–Ω—Ç–∏—Ä–æ–≤–∞–ª', '–∫—É–ø–∏–ª']):
                structured_data['actions'].append({
                    'text': text,
                    'type': 'maintenance' if any(word in text.lower() for word in ['–º–∞—Å–ª–æ', '—Ä–µ–º–æ–Ω—Ç']) else 'action'
                })
            
            # –ò—â–µ–º –ª–æ–∫–∞—Ü–∏–∏
            location_words = ['—É–ª–∏—Ü–∞', '–≤', '–Ω–∞', '–≥–∞—Ä–∞–∂', '—Å–µ—Ä–≤–∏—Å', '–æ—Ñ–∏—Å']
            if any(word in text.lower() for word in location_words):
                structured_data['locations'].append({
                    'text': text,
                    'type': 'location'
                })
        
        return structured_data
    
    def _format_structured_data(self, search_results: Dict) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è AI"""
        structured_data = self._extract_structured_data(search_results)
        formatted = []
        
        if structured_data['measurements']:
            formatted.append("–ò–ó–ú–ï–†–ï–ù–ò–Ø:")
            for measurement in structured_data['measurements']:
                formatted.append(f"- {measurement['type']}: {measurement['value']}{measurement['unit']}")
        
        if structured_data['actions']:
            formatted.append("–î–ï–ô–°–¢–í–ò–Ø:")
            for action in structured_data['actions']:
                formatted.append(f"- {action['text']}")
        
        if structured_data['amounts']:
            formatted.append("–°–£–ú–ú–´:")
            for amount in structured_data['amounts']:
                formatted.append(f"- {amount['amount']} {amount['currency']}")
        
        if structured_data['locations']:
            formatted.append("–õ–û–ö–ê–¶–ò–ò:")
            for location in structured_data['locations']:
                formatted.append(f"- {location['text']}")
        
        return "\n".join(formatted) if formatted else "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
    
    def _format_temporal_analysis(self, search_results: Dict) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è AI"""
        temporal_analysis = search_results.get('temporal_analysis')
        if not temporal_analysis:
            return "–í—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω"
        
        formatted = []
        formatted.append(f"–¢–ò–ü –ê–ù–ê–õ–ò–ó–ê: {temporal_analysis.get('type', 'unknown')}")
        
        # –ò–∑–º–µ—Ä–µ–Ω–∏—è –≤–æ –≤—Ä–µ–º–µ–Ω–∏
        measurements = temporal_analysis.get('measurements_over_time', [])
        if measurements:
            formatted.append("–ò–ó–ú–ï–†–ï–ù–ò–Ø –ü–û –í–†–ï–ú–ï–ù–ò:")
            for measurement in measurements:
                formatted.append(f"- {measurement['date']}: {measurement['value']}{measurement['unit']}")
        
        # –í—Ä–µ–º–µ–Ω–Ω–∞—è –ª–∏–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏–π
        actions = temporal_analysis.get('actions_timeline', [])
        if actions:
            formatted.append("–í–†–ï–ú–ï–ù–ù–ê–Ø –õ–ò–ù–ò–Ø –î–ï–ô–°–¢–í–ò–ô:")
            for action in actions:
                formatted.append(f"- {action['date']}: {action['action']}")
        
        # –°–≤–æ–¥–∫–∞ –ª–æ–∫–∞—Ü–∏–π
        locations = temporal_analysis.get('locations_summary', [])
        if locations:
            formatted.append("–ù–ê–ô–î–ï–ù–ù–´–ï –õ–û–ö–ê–¶–ò–ò:")
            for location in locations:
                formatted.append(f"- {location}")
        
        return "\n".join(formatted) if formatted else "–í—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
    
    async def generate_response(self, query: str, search_results: Dict) -> Dict:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–º–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–∏—Å–∫–∞"""
        try:
            topic = self._extract_topic_from_query(query)
            has_info = len(search_results['entries_found']) > 0 or len(search_results['entities_found']) > 0
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            structured_data = self._format_search_data(search_results)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è AI
            context = f"""
–ó–ê–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø: "{query}"
–¢–ï–ú–ê: {topic}
–ï–°–¢–¨ –õ–ò –ò–ù–§–û–†–ú–ê–¶–ò–Ø: {has_info}

–î–ê–ù–ù–´–ï –ò–ó –ü–û–ò–°–ö–ê:
{structured_data}

–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û–ò–°–ö–ê:
- –ù–∞–π–¥–µ–Ω–æ —Å—É—â–Ω–æ—Å—Ç–µ–π: {search_results['search_stats']['total_entities']}
- –ù–∞–π–¥–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {search_results['search_stats']['total_entries']}
- –¢–∏–ø—ã –ø–æ–∏—Å–∫–∞: {', '.join(search_results['search_stats']['search_types_used'])}

–°–¢–†–£–ö–¢–£–†–ò–†–û–í–ê–ù–ù–´–ï –î–ê–ù–ù–´–ï:
{self._format_structured_data(search_results)}

–í–†–ï–ú–ï–ù–ù–û–ô –ê–ù–ê–õ–ò–ó:
{self._format_temporal_analysis(search_results)}
"""
            
            system_prompt = self._create_system_prompt()
            
            response = await self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": context}
                ],
                temperature=0.7,  # –ù–µ–º–Ω–æ–≥–æ –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ –¥–ª—è —á–µ–ª–æ–≤–µ—á–Ω–æ—Å—Ç–∏
                max_tokens=500
            )
            
            # –ü–∞—Ä—Å–∏–º JSON –æ—Ç–≤–µ—Ç
            content = response.choices[0].message.content.strip()
            
            # –£–±–∏—Ä–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ markdown –±–ª–æ–∫–∏
            if content.startswith("```json"):
                content = content[7:]
            if content.endswith("```"):
                content = content[:-3]
            
            result = json.loads(content)
            
            # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –∏ –¥–æ–ø–æ–ª–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            return self._validate_response(result, has_info, topic)
            
        except json.JSONDecodeError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –æ—Ç AI: {e}")
            return self._fallback_response(query, search_results)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            return self._fallback_response(query, search_results)
    
    def _validate_response(self, result: Dict, has_info: bool, topic: str) -> Dict:
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∏ –¥–æ–ø–æ–ª–Ω—è–µ—Ç –æ—Ç–≤–µ—Ç AI"""
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤—Å–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –µ—Å—Ç—å
        if "response" not in result:
            result["response"] = self._generate_fallback_text(has_info, topic)
        
        if "tone" not in result:
            result["tone"] = "caring" if not has_info else "informative"
        
        if "has_info" not in result:
            result["has_info"] = has_info
        
        if "confidence" not in result:
            result["confidence"] = 0.8 if has_info else 0.6
        
        if "suggestions" not in result:
            result["suggestions"] = self._generate_suggestions(has_info, topic)
        
        return result
    
    def _generate_fallback_text(self, has_info: bool, topic: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç fallback —Ç–µ–∫—Å—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ AI"""
        if has_info:
            return f"–í–æ—Ç —á—Ç–æ —è –∑–Ω–∞—é –æ {topic} üìö"
        else:
            return f"–ü–æ–∫–∞ —è –Ω–∏—á–µ–≥–æ –Ω–µ –∑–Ω–∞—é –æ {topic}, –Ω–æ –≥–æ—Ç–æ–≤–∞ –∑–∞–ø–æ–º–Ω–∏—Ç—å! üí≠"
    
    def _generate_suggestions(self, has_info: bool, topic: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        if has_info:
            return [
                "–•–æ—á–µ—à—å —É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ –¥–µ—Ç–∞–ª–µ–π?",
                "–†–∞—Å—Å–∫–∞–∂–∏ —á—Ç–æ-—Ç–æ –Ω–æ–≤–æ–µ –æ–± —ç—Ç–æ–º!",
                "–ï—Å—Ç—å –µ—â–µ –≤–æ–ø—Ä–æ—Å—ã?"
            ]
        else:
            return [
                "–†–∞—Å—Å–∫–∞–∂–∏ –º–Ω–µ —á—Ç–æ-–Ω–∏–±—É–¥—å –æ–± —ç—Ç–æ–º!",
                "–ü–æ–¥–µ–ª–∏—Å—å –∏—Å—Ç–æ—Ä–∏–µ–π!",
                "–ß—Ç–æ –±—ã —Ç—ã —Ö–æ—Ç–µ–ª, —á—Ç–æ–±—ã —è –∑–∞–ø–æ–º–Ω–∏–ª–∞?"
            ]
    
    def _fallback_response(self, query: str, search_results: Dict) -> Dict:
        """Fallback –æ—Ç–≤–µ—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ AI"""
        topic = self._extract_topic_from_query(query)
        has_info = len(search_results['entries_found']) > 0 or len(search_results['entities_found']) > 0
        
        if has_info:
            # –ï—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è - –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∫—Ä–∞—Ç–∫–æ
            response_text = f"–í–æ—Ç —á—Ç–æ —è –∑–Ω–∞—é –æ {topic}:\n\n"
            
            if search_results['entities_found']:
                entities = [e['name'] for e in search_results['entities_found'][:3]]
                response_text += f"üè∑Ô∏è –°—É—â–Ω–æ—Å—Ç–∏: {', '.join(entities)}\n"
            
            if search_results['entries_found']:
                response_text += "üìù –ó–∞–ø–∏—Å–∏:\n"
                for entry in search_results['entries_found'][:2]:
                    text = entry['original_text'][:100] + "..." if len(entry['original_text']) > 100 else entry['original_text']
                    response_text += f"‚Ä¢ {text}\n"
        else:
            # –ù–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ - –∑–∞–±–æ—Ç–ª–∏–≤—ã–π –æ—Ç–≤–µ—Ç
            response_text = f"–ü–æ–∫–∞ —è –Ω–∏—á–µ–≥–æ –Ω–µ –∑–Ω–∞—é –æ {topic}, –Ω–æ —è –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ —Å–ª—É—à–∞—é –∏ –∑–∞–ø–æ–º–∏–Ω–∞—é –≤—Å–µ, —á—Ç–æ —Ç—ã –≥–æ–≤–æ—Ä–∏—à—å! üí≠"
        
        return {
            "response": response_text,
            "tone": "caring" if not has_info else "informative",
            "has_info": has_info,
            "confidence": 0.5,
            "suggestions": [],  # –ù–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            "fallback": True
        }
    
    def format_final_response(self, ai_response: Dict, search_results: Dict) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        response = ai_response["response"]
        
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ - —Ç–æ–ª—å–∫–æ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è
        if ai_response["tone"] == "informative":
            response = "üìä " + response
        
        # –£–±–∏—Ä–∞–µ–º –≤—Å–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã:
        # - –ù–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π
        # - –ù–µ –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (suggestions –≤—Å–µ–≥–¥–∞ –ø—É—Å—Ç–æ–π)
        # - –ù–µ –¥–æ–±–∞–≤–ª—è–µ–º –ª–∏—à–Ω–∏–µ —ç–º–æ–¥–∑–∏
        # - –ù–µ –¥–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –æ—Ç –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã—Ö —Ñ—Ä–∞–∑
        unwanted_phrases = [
            "–≠—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ —Å–∏—Å—Ç–µ–º–µ",
            "–†–µ–∫–æ–º–µ–Ω–¥—É—é –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –¥–∏–Ω–∞–º–∏–∫—É",
            "–†–µ–∫–æ–º–µ–Ω–¥—É—é –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥",
            "–°–æ–≤–µ—Ç—É—é –æ–±—Ä–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ",
            "–†–µ–∫–æ–º–µ–Ω–¥—É—é",
            "–°–æ–≤–µ—Ç—É—é",
            "–°—Ç–æ–∏—Ç",
            "–ñ–µ–ª–∞—Ç–µ–ª—å–Ω–æ"
        ]
        
        for phrase in unwanted_phrases:
            if phrase in response:
                # –£–¥–∞–ª—è–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ–π —Ñ—Ä–∞–∑–æ–π
                sentences = response.split('.')
                cleaned_sentences = []
                for sentence in sentences:
                    if phrase.lower() not in sentence.lower():
                        cleaned_sentences.append(sentence)
                response = '.'.join(cleaned_sentences).strip()
                if response and not response.endswith('.'):
                    response += '.'
        
        return response

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
async def test_response_generator():
    """–¢–µ—Å—Ç–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –æ—Ç–≤–µ—Ç–æ–≤"""
    # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à API –∫–ª—é—á DeepSeek
    api_key = "your-deepseek-api-key"
    
    generator = AIResponseGenerator(api_key)
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    test_query = "—Ä–∞—Å—Å–∫–∞–∂–∏ –æ –í–∞—Å–µ"
    test_search_results = {
        'entities_found': [
            {'id': 1, 'name': '–í–∞—Å—è', 'type': 'person', 'mention_count': 3}
        ],
        'entries_found': [
            {'id': 1, 'original_text': '–í—Å—Ç—Ä–µ—Ç–∏–ª –í–∞—Å—é –≤ –∞–≤—Ç–æ—Å–µ—Ä–≤–∏—Å–µ', 'entity_names': '–í–∞—Å—è', 'tag_names': '–ª—é–¥–∏,–º–µ—Å—Ç–∞'},
            {'id': 2, 'original_text': '–í–∞—Å—è –∫—É–ø–∏–ª –Ω–æ–≤—É—é –º–∞—à–∏–Ω—É', 'entity_names': '–í–∞—Å—è', 'tag_names': '–ª—é–¥–∏,–∞–≤—Ç–æ–º–æ–±–∏–ª–∏'}
        ],
        'search_stats': {'total_entities': 1, 'total_entries': 2, 'search_types_used': ['entities_person']}
    }
    
    result = await generator.generate_response(test_query, test_search_results)
    print(f"–û—Ç–≤–µ—Ç: {result['response']}")
    print(f"–¢–æ–Ω: {result['tone']}")
    print(f"–ï—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: {result['has_info']}")

if __name__ == "__main__":
    asyncio.run(test_response_generator())

"""
–£–º–Ω—ã–π –ø–æ–∏—Å–∫ –∏ –∞–Ω–∞–ª–∏–∑ –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ "—Ä–∞—Å—Å–∫–∞–∂–∏"
–ü—Ä–æ–µ–∫—Ç "–í—Ç–æ—Ä–æ–π –º–æ–∑–≥" - –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π –≥–æ–ª–æ—Å–æ–≤–æ–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç
"""

import json
import re
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta
import logging
from database import DatabaseManager
from ai_query_parser import AIQueryParser

logger = logging.getLogger(__name__)

class SmartSearchEngine:
    """–î–≤–∏–∂–æ–∫ —É–º–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∑–∞–ø–∏—Å–µ–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    
    def __init__(self, db_manager: DatabaseManager, ai_api_key: str = None):
        self.db = db_manager
        self.ai_parser = AIQueryParser(ai_api_key) if ai_api_key else None
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        self.query_patterns = {
            'person': [
                r'–æ\s+(\w+)', r'–ø—Ä–æ\s+(\w+)', r'(\w+)\s+–∫—Ç–æ', r'–∫—Ç–æ\s+—Ç–∞–∫–æ–π\s+(\w+)',
                r'(\w+)\s+—á—Ç–æ', r'—á—Ç–æ\s+–∑–Ω–∞–µ—à—å\s+–æ\s+(\w+)', r'(\w+)\s+—Ä–∞—Å—Å–∫–∞–∂–∏'
            ],
            'place': [
                r'–≤\s+(\w+)', r'–≥–¥–µ\s+(\w+)', r'(\w+)\s+–≥–¥–µ', r'–º–µ—Å—Ç–æ\s+(\w+)',
                r'–ª–æ–∫–∞—Ü–∏—è\s+(\w+)', r'–∞–¥—Ä–µ—Å\s+(\w+)'
            ],
            'object': [
                r'(\w+)\s+—á—Ç–æ', r'—á—Ç–æ\s+—Ç–∞–∫–æ–µ\s+(\w+)', r'–æ–±—ä–µ–∫—Ç\s+(\w+)',
                r'–≤–µ—â—å\s+(\w+)', r'–ø—Ä–µ–¥–º–µ—Ç\s+(\w+)'
            ],
            'event': [
                r'—Å–æ–±—ã—Ç–∏–µ\s+(\w+)', r'–≤—Å—Ç—Ä–µ—á–∞\s+(\w+)', r'(\w+)\s+–∫–æ–≥–¥–∞',
                r'–∫–æ–≥–¥–∞\s+(\w+)', r'–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ\s+(\w+)'
            ],
            'task': [
                r'–∑–∞–¥–∞—á–∞\s+(\w+)', r'–¥–µ–ª–æ\s+(\w+)', r'(\w+)\s+–Ω—É–∂–Ω–æ',
                r'–Ω—É–∂–Ω–æ\s+(\w+)', r'–ø–æ–∫—É–ø–∫–∞\s+(\w+)'
            ],
            'reminder': [
                r'–Ω–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ\s+(\w+)', r'(\w+)\s+–Ω–∞–ø–æ–º–Ω–∏', r'–Ω–∞–ø–æ–º–Ω–∏\s+(\w+)',
                r'–≤–∞–∂–Ω–æ\s+(\w+)', r'—Å—Ä–æ—á–Ω–æ\s+(\w+)'
            ]
        }
        
        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–ª–æ–≤ (–ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –±–∞–∑–æ–≤–æ–π —Ñ–æ—Ä–º–µ)
        self.word_normalization = {
            '–≤–∞—Å–µ': '–≤–∞—Å—è', '–≤–∞—Å—é': '–≤–∞—Å—è', '–≤–∞—Å–∏': '–≤–∞—Å—è',
            '–º–∞—à–∏–Ω–µ': '–º–∞—à–∏–Ω–∞', '–º–∞—à–∏–Ω—É': '–º–∞—à–∏–Ω–∞', '–º–∞—à–∏–Ω—ã': '–º–∞—à–∏–Ω–∞',
            '—Ä–∞–±–æ—Ç–µ': '—Ä–∞–±–æ—Ç–∞', '—Ä–∞–±–æ—Ç—É': '—Ä–∞–±–æ—Ç–∞', '—Ä–∞–±–æ—Ç—ã': '—Ä–∞–±–æ—Ç–∞',
            '–≤—Å—Ç—Ä–µ—á–∞—Ö': '–≤—Å—Ç—Ä–µ—á–∞', '–≤—Å—Ç—Ä–µ—á–∏': '–≤—Å—Ç—Ä–µ—á–∞', '–≤—Å—Ç—Ä–µ—á—É': '–≤—Å—Ç—Ä–µ—á–∞',
            '–º–æ–ª–æ–∫–µ': '–º–æ–ª–æ–∫–æ', '–º–æ–ª–æ–∫–∞': '–º–æ–ª–æ–∫–æ', '–º–æ–ª–æ–∫–æ': '–º–æ–ª–æ–∫–æ',
            '–Ω–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è': '–Ω–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ', '–Ω–∞–ø–æ–º–∏–Ω–∞–Ω–∏–π': '–Ω–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ',
            '–≤–∞–±–∏': '–≤–∞–±–∏ —Å–∞–±–∏', '—Å–∞–±–∏': '–≤–∞–±–∏ —Å–∞–±–∏'  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è —Å–æ—Å—Ç–∞–≤–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π
        }
        
        # –û–±—Ä–∞—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤—Å–µ—Ö —Ñ–æ—Ä–º —Å–ª–æ–≤–∞
        self.reverse_normalization = {}
        for form, base in self.word_normalization.items():
            if base not in self.reverse_normalization:
                self.reverse_normalization[base] = []
            self.reverse_normalization[base].append(form)
            self.reverse_normalization[base].append(base)  # –î–æ–±–∞–≤–ª—è–µ–º –∏ –±–∞–∑–æ–≤—É—é —Ñ–æ—Ä–º—É
        
        # –°–æ—Å—Ç–∞–≤–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –∏—Å–∫–∞—Ç—å –∫–∞–∫ –µ–¥–∏–Ω–æ–µ —Ü–µ–ª–æ–µ
        self.compound_names = {
            '–≤–∞–±–∏ —Å–∞–±–∏': ['–≤–∞–±–∏', '—Å–∞–±–∏'],
            '—Ä–∞–∫–∏ —Ä–æ–ª–ª': ['—Ä–∞–∫–∏', '—Ä–æ–ª–ª'],
            '—Ö–∞–¥–∂–∏ –º–∞–º—Å—É—Ä–æ–≤–∞': ['—Ö–∞–¥–∂–∏', '–º–∞–º—Å—É—Ä–æ–≤–∞'],
            '–∫–æ–ª–∫–∞ –∫–∏—Å–∞–µ–≤–∞': ['–∫–æ–ª–∫–∞', '–∫–∏—Å–∞–µ–≤–∞'],
            '–≤–ª–∞–¥ —Ç–æ—Ç–∏–µ–≤': ['–≤–ª–∞–¥', '—Ç–æ—Ç–∏–µ–≤']
        }
        
        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        self.stop_words = {
            '—á—Ç–æ', '–∫–∞–∫', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–∫—Ç–æ', '—Ä–∞—Å—Å–∫–∞–∂–∏', '–ø–æ–∫–∞–∂–∏', '–∑–Ω–∞–µ—à—å',
            '–ª–∏', '–æ', '–ø—Ä–æ', '–≤', '–Ω–∞', '—Å', '—É', '–¥–ª—è', '–æ—Ç', '–¥–æ', '–∑–∞', '–ø–æ',
            '–∏', '–∞', '–Ω–æ', '–∏–ª–∏', '–µ—Å–ª–∏', '—á—Ç–æ–±—ã', '–ø–æ—Ç–æ–º—É', '—á—Ç–æ', '—Ç–∞–∫–∂–µ',
            '–µ—â–µ', '—É–∂–µ', '–≤—Å–µ', '–≤—Å–µ–≥–æ', '–≤—Å–µ—Ö', '–≤—Å–µ–º', '–≤—Å–µ–º–∏', '–≤—Å–µ–º—É',
            '—ç—Ç–æ', '—ç—Ç–æ–≥–æ', '—ç—Ç–æ–º—É', '—ç—Ç–∏–º', '—ç—Ç–æ–º', '—ç—Ç–∞', '—ç—Ç–æ–π', '—ç—Ç—É',
            '—ç—Ç–æ—Ç', '—ç—Ç–∏', '—ç—Ç–∏—Ö', '—ç—Ç–∏–º', '—ç—Ç–∏–º–∏', '—ç—Ç–∏–º', '—ç—Ç–æ—Ç'
        }
    
    def parse_query(self, query: str) -> Dict[str, List[str]]:
        """–ü–∞—Ä—Å–∏—Ç –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞"""
        query_lower = query.lower().strip()
        extracted = {
            'person': [],
            'place': [],
            'object': [],
            'event': [],
            'task': [],
            'reminder': [],
            'general': []
        }
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ—Å—Ç–∞–≤–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
        compound_found = []
        for compound_name, parts in self.compound_names.items():
            if all(part in query_lower for part in parts):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —á–∞—Å—Ç–∏ –∏–¥—É—Ç –ø–æ–¥—Ä—è–¥ –∏–ª–∏ –±–ª–∏–∑–∫–æ –¥—Ä—É–≥ –∫ –¥—Ä—É–≥—É
                compound_pattern = r'\b' + r'\s+'.join(parts) + r'\b'
                if re.search(compound_pattern, query_lower):
                    compound_found.append(compound_name)
                    # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ—Å—Ç–∞–≤–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –≤ –æ–±—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
                    extracted['general'].append(compound_name)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –ø–æ —Ç–∏–ø–∞–º
        for entity_type, patterns in self.query_patterns.items():
            for pattern in patterns:
                matches = re.findall(pattern, query_lower)
                for match in matches:
                    if isinstance(match, tuple):
                        match = match[0]
                    if match and len(match) > 2 and match not in self.stop_words:
                        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å–ª–æ–≤–æ
                        normalized_word = self.word_normalization.get(match, match)
                        # –ï—Å–ª–∏ —ç—Ç–æ —á–∞—Å—Ç—å —Å–æ—Å—Ç–∞–≤–Ω–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä–æ–µ —É–∂–µ –Ω–∞–π–¥–µ–Ω–æ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                        if not any(part in match for compound in compound_found for part in self.compound_names[compound]):
                            extracted[entity_type].append(normalized_word)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–±—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        words = re.findall(r'\b\w+\b', query_lower)
        for word in words:
            if (len(word) > 2 and 
                word not in self.stop_words and 
                not any(word in extracted[entity_type] for entity_type in extracted if entity_type != 'general')):
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å–ª–æ–≤–æ
                normalized_word = self.word_normalization.get(word, word)
                # –ï—Å–ª–∏ —ç—Ç–æ —á–∞—Å—Ç—å —Å–æ—Å—Ç–∞–≤–Ω–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä–æ–µ —É–∂–µ –Ω–∞–π–¥–µ–Ω–æ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                if not any(part in word for compound in compound_found for part in self.compound_names[compound]):
                    extracted['general'].append(normalized_word)
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        for entity_type in extracted:
            extracted[entity_type] = list(set(extracted[entity_type]))
        
        return extracted
    
    async def parse_query_ai(self, query: str) -> Dict[str, List[str]]:
        """–ü–∞—Ä—Å–∏—Ç –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –ø–æ–º–æ—â—å—é AI"""
        if not self.ai_parser:
            # Fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
            return self.parse_query(query)
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º AI –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
            parsed_result = await self.ai_parser.parse_query(query)
            search_terms = self.ai_parser.extract_search_terms(parsed_result)
            
            logger.info(f"AI –ø–∞—Ä—Å–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞ '{query}': {len(parsed_result.get('entities', []))} —Å—É—â–Ω–æ—Å—Ç–µ–π")
            return search_terms
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ AI –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            # Fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
            return self.parse_query(query)
    
    def search_entities(self, user_id: int, keywords: List[str], entity_type: str = None) -> List[Dict]:
        """–ü–æ–∏—Å–∫ —Å—É—â–Ω–æ—Å—Ç–µ–π –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        try:
            with self.db.get_connection() as conn:
                cursor = conn.cursor()
                
                # –°–æ–∑–¥–∞–µ–º —É—Å–ª–æ–≤–∏—è –ø–æ–∏—Å–∫–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
                search_conditions = []
                params = []
                
                for kw in keywords:
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –≤ –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä
                    normalized_kw = kw.lower().strip()
                    
                    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
                    search_forms = [normalized_kw]
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é —Ñ–æ—Ä–º—É
                    if normalized_kw in self.word_normalization:
                        search_forms.append(self.word_normalization[normalized_kw])
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ —Ñ–æ—Ä–º—ã –∏–∑ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è
                    for base_form, all_forms in self.reverse_normalization.items():
                        if normalized_kw in all_forms:
                            search_forms.extend(all_forms)
                    
                    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
                    search_forms = list(set(search_forms))
                    
                    # –°–æ–∑–¥–∞–µ–º —É—Å–ª–æ–≤–∏—è –ø–æ–∏—Å–∫–∞ –¥–ª—è –≤—Å–µ—Ö —Ñ–æ—Ä–º
                    for form in search_forms:
                        # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (–≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
                        search_conditions.append("name = ?")
                        params.append(form)
                        
                        # –ù–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å —Ñ–æ—Ä–º—ã
                        search_conditions.append("name LIKE ?")
                        params.append(f"{form}%")
                        
                        # –°–æ–¥–µ—Ä–∂–∏—Ç —Ñ–æ—Ä–º—É
                        search_conditions.append("name LIKE ?")
                        params.append(f"%{form}%")
                    
                    # –î–ª—è —Å–æ—Å—Ç–∞–≤–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π - –∏—â–µ–º –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –æ—Ç–¥–µ–ª—å–Ω–æ
                    if ' ' in normalized_kw:
                        words = normalized_kw.split()
                        for word in words:
                            search_conditions.append("name LIKE ?")
                            params.append(f"%{word}%")
                            # –¢–∞–∫–∂–µ –∏—â–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ñ–æ—Ä–º—ã
                            if word in self.word_normalization:
                                normalized_word = self.word_normalization[word]
                                search_conditions.append("name LIKE ?")
                                params.append(f"%{normalized_word}%")
                
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º —É—Å–ª–æ–≤–∏—è
                where_condition = " OR ".join(search_conditions)
                
                if entity_type:
                    # –ü–æ–∏—Å–∫ –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É —Ç–∏–ø—É —Å—É—â–Ω–æ—Å—Ç–∏
                    query = f"""
                        SELECT DISTINCT e.*, COUNT(ee.entry_id) as mention_count,
                               CASE 
                                   WHEN e.name = ? THEN 3
                                   WHEN e.name LIKE ? THEN 2
                                   ELSE 1
                               END as match_priority
                        FROM entities e
                        LEFT JOIN entry_entities ee ON e.id = ee.entity_id
                        WHERE e.user_id = ? AND e.type = ? AND ({where_condition})
                        GROUP BY e.id
                        ORDER BY match_priority DESC, mention_count DESC, e.name
                    """
                    # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π –∫–ª—é—á–µ–≤–æ–π —ç–ª–µ–º–µ–Ω—Ç)
                    priority_params = [keywords[0].lower().strip(), f"{keywords[0].lower().strip()}%"] if keywords else ["", ""]
                    final_params = priority_params + [user_id, entity_type] + params
                else:
                    # –ü–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º —Ç–∏–ø–∞–º
                    query = f"""
                        SELECT DISTINCT e.*, COUNT(ee.entry_id) as mention_count,
                               CASE 
                                   WHEN e.name = ? THEN 3
                                   WHEN e.name LIKE ? THEN 2
                                   ELSE 1
                               END as match_priority
                        FROM entities e
                        LEFT JOIN entry_entities ee ON e.id = ee.entity_id
                        WHERE e.user_id = ? AND ({where_condition})
                        GROUP BY e.id
                        ORDER BY match_priority DESC, mention_count DESC, e.name
                    """
                    # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π –∫–ª—é—á–µ–≤–æ–π —ç–ª–µ–º–µ–Ω—Ç)
                    priority_params = [keywords[0].lower().strip(), f"{keywords[0].lower().strip()}%"] if keywords else ["", ""]
                    final_params = priority_params + [user_id] + params
                
                cursor.execute(query, final_params)
                rows = cursor.fetchall()
                return [dict(row) for row in rows]
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Å—É—â–Ω–æ—Å—Ç–µ–π: {e}")
            return []
    
    def search_entries_by_entities(self, user_id: int, entity_ids: List[int], limit: int = 10) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –∑–∞–ø–∏—Å–µ–π –ø–æ —Å—É—â–Ω–æ—Å—Ç—è–º"""
        if not entity_ids:
            return []
        
        try:
            with self.db.get_connection() as conn:
                cursor = conn.cursor()
                placeholders = ','.join(['?' for _ in entity_ids])
                query = f"""
                    SELECT DISTINCT e.*, 
                           GROUP_CONCAT(DISTINCT ent.name) as entity_names,
                           GROUP_CONCAT(DISTINCT t.name) as tag_names
                    FROM entries e
                    JOIN entry_entities ee ON e.id = ee.entry_id
                    JOIN entities ent ON ee.entity_id = ent.id
                    LEFT JOIN entry_tags et ON e.id = et.entry_id
                    LEFT JOIN tags t ON et.tag_id = t.id
                    WHERE e.user_id = ? AND ee.entity_id IN ({placeholders})
                    GROUP BY e.id
                    ORDER BY e.created_at DESC
                    LIMIT ?
                """
                params = [user_id] + entity_ids + [limit]
                
                cursor.execute(query, params)
                rows = cursor.fetchall()
                return [dict(row) for row in rows]
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –∑–∞–ø–∏—Å–µ–π –ø–æ —Å—É—â–Ω–æ—Å—Ç—è–º: {e}")
            return []
    
    def search_entries_by_text(self, user_id: int, keywords: List[str], limit: int = 10) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –∑–∞–ø–∏—Å–µ–π –ø–æ —Ç–µ–∫—Å—Ç—É (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –≤ –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä)"""
        if not keywords:
            return []
        
        try:
            with self.db.get_connection() as conn:
                cursor = conn.cursor()
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä
                normalized_keywords = [kw.lower().strip() for kw in keywords]
                placeholders = ' OR '.join(['original_text LIKE ?' for _ in normalized_keywords])
                query = f"""
                    SELECT e.*, 
                           GROUP_CONCAT(DISTINCT ent.name) as entity_names,
                           GROUP_CONCAT(DISTINCT t.name) as tag_names
                    FROM entries e
                    LEFT JOIN entry_entities ee ON e.id = ee.entry_id
                    LEFT JOIN entities ent ON ee.entity_id = ent.id
                    LEFT JOIN entry_tags et ON e.id = et.entry_id
                    LEFT JOIN tags t ON et.tag_id = t.id
                    WHERE e.user_id = ? AND ({placeholders})
                    GROUP BY e.id
                    ORDER BY e.created_at DESC
                    LIMIT ?
                """
                params = [user_id] + [f'%{kw}%' for kw in normalized_keywords] + [limit]
                
                cursor.execute(query, params)
                rows = cursor.fetchall()
                return [dict(row) for row in rows]
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –∑–∞–ø–∏—Å–µ–π –ø–æ —Ç–µ–∫—Å—Ç—É: {e}")
            return []
    
    def search_entries_by_date(self, user_id: int, target_date: str, limit: int = 10) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –∑–∞–ø–∏—Å–µ–π –ø–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–æ–π –¥–∞—Ç–µ"""
        try:
            with self.db.get_connection() as conn:
                cursor = conn.cursor()
                query = """
                    SELECT e.*, 
                           GROUP_CONCAT(DISTINCT ent.name) as entity_names,
                           GROUP_CONCAT(DISTINCT t.name) as tag_names
                    FROM entries e
                    LEFT JOIN entry_entities ee ON e.id = ee.entry_id
                    LEFT JOIN entities ent ON ee.entity_id = ent.id
                    LEFT JOIN entry_tags et ON e.id = et.entry_id
                    LEFT JOIN tags t ON et.tag_id = t.id
                    WHERE e.user_id = ? AND e.metadata LIKE ?
                    GROUP BY e.id
                    ORDER BY e.created_at DESC
                    LIMIT ?
                """
                # –ò—â–µ–º –ø–æ –¥–∞—Ç–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ YYYY-MM-DD
                date_pattern = f'%{target_date}%'
                cursor.execute(query, (user_id, date_pattern, limit))
                rows = cursor.fetchall()
                return [dict(row) for row in rows]
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ –¥–∞—Ç–µ: {e}")
            return []
    
    def get_related_entities(self, user_id: int, entity_id: int) -> List[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏"""
        try:
            with self.db.get_connection() as conn:
                cursor = conn.cursor()
                query = """
                    SELECT DISTINCT e2.*, COUNT(*) as co_occurrence_count
                    FROM entities e1
                    JOIN entry_entities ee1 ON e1.id = ee1.entity_id
                    JOIN entry_entities ee2 ON ee1.entry_id = ee2.entry_id
                    JOIN entities e2 ON ee2.entity_id = e2.id
                    WHERE e1.id = ? AND e2.id != ? AND e2.user_id = ?
                    GROUP BY e2.id
                    ORDER BY co_occurrence_count DESC
                    LIMIT 5
                """
                cursor.execute(query, (entity_id, entity_id, user_id))
                rows = cursor.fetchall()
                return [dict(row) for row in rows]
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π: {e}")
            return []
    
    def get_recent_entries(self, user_id: int, days: int = 7, limit: int = 5) -> List[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –Ω–µ–¥–∞–≤–Ω–∏–µ –∑–∞–ø–∏—Å–∏"""
        try:
            with self.db.get_connection() as conn:
                cursor = conn.cursor()
                cutoff_date = datetime.now() - timedelta(days=days)
                query = """
                    SELECT e.*, 
                           GROUP_CONCAT(DISTINCT ent.name) as entity_names,
                           GROUP_CONCAT(DISTINCT t.name) as tag_names
                    FROM entries e
                    LEFT JOIN entry_entities ee ON e.id = ee.entry_id
                    LEFT JOIN entities ent ON ee.entity_id = ent.id
                    LEFT JOIN entry_tags et ON e.id = et.tag_id
                    LEFT JOIN tags t ON et.tag_id = t.id
                    WHERE e.user_id = ? AND e.created_at >= ?
                    GROUP BY e.id
                    ORDER BY e.created_at DESC
                    LIMIT ?
                """
                cursor.execute(query, (user_id, cutoff_date, limit))
                rows = cursor.fetchall()
                return [dict(row) for row in rows]
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–µ–¥–∞–≤–Ω–∏—Ö –∑–∞–ø–∏—Å–µ–π: {e}")
            return []
    
    async def search_comprehensive(self, user_id: int, query: str) -> Dict:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º AI-–ø–∞—Ä—Å–∏–Ω–≥ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        if self.ai_parser:
            parsed_result = await self.ai_parser.parse_query(query)
            parsed_query = self.ai_parser.extract_search_terms(parsed_result)
            is_analytical = self.ai_parser.is_analytical_query(parsed_result)
            analytical_type = self.ai_parser.get_analytical_type(parsed_result)
        else:
            parsed_query = self.parse_query(query)
            is_analytical = False
            analytical_type = None
            parsed_result = None
        
        results = {
            'query': query,
            'parsed_keywords': parsed_query,
            'parsed_result': parsed_result,
            'is_analytical': is_analytical,
            'analytical_type': analytical_type,
            'entities_found': [],
            'entries_found': [],
            'related_entities': [],
            'recent_entries': [],
            'search_stats': {
                'total_entities': 0,
                'total_entries': 0,
                'search_types_used': []
            }
        }
        
        # –ü–æ–∏—Å–∫ –ø–æ —Å—É—â–Ω–æ—Å—Ç—è–º
        for entity_type, keywords in parsed_query.items():
            if keywords and entity_type != 'general':
                entities = self.search_entities(user_id, keywords, entity_type)
                results['entities_found'].extend(entities)
                results['search_stats']['search_types_used'].append(f'entities_{entity_type}')
        
        # –ü–æ–∏—Å–∫ –ø–æ –æ–±—â–∏–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        if parsed_query['general']:
            general_entities = self.search_entities(user_id, parsed_query['general'])
            results['entities_found'].extend(general_entities)
            results['search_stats']['search_types_used'].append('entities_general')
        
        # –ù–û–í–û–ï: –ü–æ–∏—Å–∫ –ø–æ –¥–∞—Ç–∞–º –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—Å —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —É–∫–∞–∑–∞–Ω–∏—è
        if parsed_result and parsed_result.get('entities'):
            temporal_entities = [e for e in parsed_result['entities'] if e.get('type') == 'temporal']
            if temporal_entities:
                # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
                date_result = self.db.date_parser.parse_text(query)
                if date_result['confidence'] > 0.5 and date_result['datetime']:
                    target_date = date_result['date_string'][:10]  # YYYY-MM-DD
                    date_entries = self.search_entries_by_date(user_id, target_date)
                    results['entries_found'].extend(date_entries)
                    results['search_stats']['search_types_used'].append('date_search')
                    logger.info(f"üìÖ –ü–æ–∏—Å–∫ –ø–æ –¥–∞—Ç–µ {target_date}: –Ω–∞–π–¥–µ–Ω–æ {len(date_entries)} –∑–∞–ø–∏—Å–µ–π")
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã —Å—É—â–Ω–æ—Å—Ç–µ–π
        seen_entities = set()
        unique_entities = []
        for entity in results['entities_found']:
            if entity['id'] not in seen_entities:
                seen_entities.add(entity['id'])
                unique_entities.append(entity)
        results['entities_found'] = unique_entities
        
        # –ü–æ–∏—Å–∫ –∑–∞–ø–∏—Å–µ–π –ø–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–º —Å—É—â–Ω–æ—Å—Ç—è–º
        if results['entities_found']:
            entity_ids = [e['id'] for e in results['entities_found']]
            entries = self.search_entries_by_entities(user_id, entity_ids)
            results['entries_found'].extend(entries)
            results['search_stats']['search_types_used'].append('entries_by_entities')
        
        # –ü–æ–∏—Å–∫ –∑–∞–ø–∏—Å–µ–π –ø–æ —Ç–µ–∫—Å—Ç—É
        all_keywords = []
        for keywords in parsed_query.values():
            all_keywords.extend(keywords)
        
        if all_keywords:
            text_entries = self.search_entries_by_text(user_id, all_keywords)
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ –∑–∞–ø–∏—Å–∏
            existing_entry_ids = {e['id'] for e in results['entries_found']}
            for entry in text_entries:
                if entry['id'] not in existing_entry_ids:
                    results['entries_found'].append(entry)
            results['search_stats']['search_types_used'].append('entries_by_text')
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ –¥–ª—è —Ç–æ–ø-—Å—É—â–Ω–æ—Å—Ç–µ–π
        if results['entities_found']:
            top_entity = results['entities_found'][0]
            related = self.get_related_entities(user_id, top_entity['id'])
            results['related_entities'] = related
        
        # –ü–æ–ª—É—á–∞–µ–º –Ω–µ–¥–∞–≤–Ω–∏–µ –∑–∞–ø–∏—Å–∏ –∫–∞–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç
        recent = self.get_recent_entries(user_id, days=7, limit=3)
        results['recent_entries'] = recent
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        results['search_stats']['total_entities'] = len(results['entities_found'])
        results['search_stats']['total_entries'] = len(results['entries_found'])
        
        # –ï—Å–ª–∏ —ç—Ç–æ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å, –¥–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑
        if is_analytical and analytical_type:
            results['temporal_analysis'] = self._analyze_temporal_data(results['entries_found'], analytical_type)
        
        return results
    
    def _analyze_temporal_data(self, entries: List[Dict], analytical_type: str) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
        temporal_analysis = {
            'type': analytical_type,
            'measurements_over_time': [],
            'actions_timeline': [],
            'locations_summary': [],
            'amounts_summary': []
        }
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –∑–∞–ø–∏—Å–∏ –ø–æ –¥–∞—Ç–∞–º
        entries_by_date = {}
        for entry in entries:
            date = entry.get('created_at', '')
            if date:
                date_key = date[:10]  # YYYY-MM-DD
                if date_key not in entries_by_date:
                    entries_by_date[date_key] = []
                entries_by_date[date_key].append(entry)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–∑–º–µ—Ä–µ–Ω–∏—è –≤–æ –≤—Ä–µ–º–µ–Ω–∏
        if analytical_type == 'weight_tracking':
            for date, day_entries in entries_by_date.items():
                for entry in day_entries:
                    text = entry.get('original_text', '')
                    # –ò—â–µ–º –≤–µ—Å –≤ —Ç–µ–∫—Å—Ç–µ
                    import re
                    weight_match = re.search(r'(\d+)\s*(–∫–≥|–∫–∏–ª–æ–≥—Ä–∞–º–º)', text, re.IGNORECASE)
                    if weight_match:
                        temporal_analysis['measurements_over_time'].append({
                            'date': date,
                            'value': int(weight_match.group(1)),
                            'unit': 'kg',
                            'text': text
                        })
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–µ–π—Å—Ç–≤–∏—è
        elif analytical_type == 'maintenance_history':
            for date, day_entries in entries_by_date.items():
                for entry in day_entries:
                    text = entry.get('original_text', '')
                    if any(word in text.lower() for word in ['–º–µ–Ω—è–ª', '–ø–æ—á–∏–Ω–∏–ª', '—Ä–µ–º–æ–Ω—Ç–∏—Ä–æ–≤–∞–ª']):
                        temporal_analysis['actions_timeline'].append({
                            'date': date,
                            'action': text,
                            'type': 'maintenance'
                        })
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ª–æ–∫–∞—Ü–∏–∏
        elif analytical_type == 'location_search':
            locations = set()
            for entry in entries:
                text = entry.get('original_text', '')
                # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ª–æ–∫–∞—Ü–∏–π
                location_words = ['—É–ª–∏—Ü–∞', '–≤', '–Ω–∞', '–≥–∞—Ä–∞–∂', '—Å–µ—Ä–≤–∏—Å', '–æ—Ñ–∏—Å']
                for word in location_words:
                    if word in text.lower():
                        locations.add(text)
            temporal_analysis['locations_summary'] = list(locations)
        
        return temporal_analysis
